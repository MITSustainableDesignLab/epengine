import logging
import tempfile

from archetypal import IDF
from archetypal.idfclass.sql import Sql
from hatchet_sdk.context import Context

from epengine.hatchet import hatchet
from epengine.models.configs import SimulationSpec
from epengine.utils.results import postprocess, serialize_df_dict

logger = logging.getLogger(__name__)


# TODO: This could be generated by a class method in the SimulationSpec class
# but should it?
@hatchet.workflow(
    name="simulate_epw_idf",
    on_events=["simulation:run_artifacts"],
    timeout="4m",
    version="0.2",
)
class Simulate:
    @hatchet.step(name="simulate", timeout="4m")
    def simulate(self, context: Context):
        data = context.workflow_input()
        data["hcontext"] = context
        spec = SimulationSpec(**data)
        with tempfile.TemporaryDirectory() as tmpdir:
            idf = IDF(spec.idf_path, epw=spec.epw_path, output_directory=tmpdir)
            context.log(f"Simulating {spec.idf_path}...")
            idf.simulate()
            sql = Sql(idf.sql_file)
            index_data = spec.model_dump(mode="json", exclude_none=True)
            workflow_run_id = context.workflow_run_id()
            # TODO: pull in spawn index
            index_data["workflow_run_id"] = workflow_run_id
            dfs = postprocess(
                sql,
                index_data=index_data,
                tabular_lookups=[("AnnualBuildingUtilityPerformanceSummary", "End Uses")],
            )
        dfs = serialize_df_dict(dfs)

        return dfs
